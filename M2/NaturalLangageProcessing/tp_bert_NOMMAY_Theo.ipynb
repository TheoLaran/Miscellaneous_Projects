{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "4e-cM3FPUE-V"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU is available.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import transformers\n",
    "\n",
    "# Managing arrays\n",
    "import numpy as np\n",
    "\n",
    "# load the TensorBoard notebook extension\n",
    "# %load_ext tensorboard\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "  print(\"GPU is available.\")\n",
    "  device = torch.cuda.current_device()\n",
    "\n",
    "else:\n",
    "  print(\"Will work on CPU\")\n",
    "\n",
    "\n",
    "## IMPROVE THE MODEL\n",
    "\n",
    "# # SOLUTION 1 (trivial): increase training epochs\n",
    "\n",
    "# # SOLUTION 3: let's see what students can do !\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "## DATA\n",
    "\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "\n",
    "categories = [\n",
    "    'comp.windows.x',\n",
    "    'sci.med',\n",
    "    'soc.religion.christian',\n",
    "    'talk.politics.guns',\n",
    "]\n",
    "\n",
    "# download data if not already present in data_home\n",
    "trainset = fetch_20newsgroups(subset='train', categories=categories, shuffle=True, random_state=42, data_home='./scikit_learn_data')\n",
    "testset = fetch_20newsgroups(subset='test', categories=categories, shuffle=True, random_state=42, data_home='./scikit_learn_data')\n",
    "\n",
    "# define input data and labels for training and testing\n",
    "x_train = trainset.data\n",
    "y_train = trainset.target\n",
    "x_test = testset.data\n",
    "y_test = testset.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "taille du jeu de données: \n",
      " \t 3885 posts de forum en total\n",
      "\t 2332 posts pour le training\n",
      "\t\t 593 comp.windows.x\n",
      "\t 1553 posts pour le test\n",
      "\t\t 594 sci.med\n",
      "\t 1553 posts pour le test\n",
      "\t\t 599 soc.religion.christian\n",
      "\t 1553 posts pour le test\n",
      "\t\t 546 talk.politics.guns\n",
      "\t 1553 posts pour le test\n",
      "\t\t 395 comp.windows.x\n",
      "\t\t 396 sci.med\n",
      "\t\t 398 soc.religion.christian\n",
      "\t\t 364 talk.politics.guns\n",
      "\n",
      "\n",
      "EXEMPLE: \n",
      "\n",
      "From: gilham@csl.sri.com (Fred Gilham)\n",
      "Subject: Poem\n",
      "Organization: Computer Science Lab, SRI International, Menlo Park, CA.\n",
      "Lines: 145\n",
      "\n",
      "\n",
      "          The Sophomore\n",
      "          (Romans 1:22)\n",
      "\n",
      "The sophomore says, ``What is truth?''\n",
      "and turns to bask in the admiration of his peers.\n",
      "\n",
      "How modern how daring how liberating\n",
      "How modern how daring how liberating\n",
      "they chant\n",
      "\n",
      "The sophomore, being American\n",
      "Doesn't know\n",
      "That his ``question''\n",
      "\n",
      "   modern\n",
      "       skeptical\n",
      "           cynical\n",
      "\n",
      "Was asked before, by a\n",
      "\n",
      "   modern\n",
      "       skeptical\n",
      "           cynical\n",
      "   urbane cosmopolitan\n",
      "\n",
      "Politician (appointed not elected)\n",
      "Who happened to live two thousand years ago.\n",
      "\n",
      "Like many politicians he cared\n",
      "\n",
      "    Less about ideals\n",
      "         than results\n",
      "\n",
      "    Less about ends\n",
      "         than means\n",
      "\n",
      "    Less about anything\n",
      "         than keeping his job\n",
      "              (and his head).\n",
      "\n",
      "We might call him\n",
      "A bit brutal\n",
      "Though `firm' would be kinder\n",
      "(And no doubt Stalin, who let nobody go, laughed at his laxness)\n",
      "He didn't like his job; perhaps he no longer hoped for better\n",
      "(Nor feared worse, except regarding his head).\n",
      "\n",
      "And when these wily Jews\n",
      "With their heads-I-win, tails-you-lose\n",
      "     conundrums\n",
      "Brought forth their madman,\n",
      "His first impulse was to play the Roman:\n",
      "``I find nothing wrong with him,\n",
      "  See to it yourselves.''\n",
      "\n",
      "But when they mentioned `King' and `Caesar'\n",
      "His heart froze.\n",
      "\n",
      "If he killed their madman\n",
      "    He'd start a riot\n",
      "         and lose his job\n",
      "             (and his head)\n",
      "\n",
      "If he saved the King of the Jews\n",
      "    He'd piss off Caesar\n",
      "         and lose his job\n",
      "             (and his head)\n",
      "\n",
      "And when his wife told him to have\n",
      "   Nothing to do with the righteous lout\n",
      "She didn't tell him anything\n",
      "   He hadn't already figured out.\n",
      "\n",
      "So he punted.\n",
      "\n",
      "``Not my jurisdiction!  Take him to see Herod!''\n",
      "(who just happened to be in town....)\n",
      "\n",
      "Herod appreciated the courtesy\n",
      "But wasn't worried\n",
      "        And sent the sharp-tongued fool\n",
      "     (Who suddenly didn't have much to say,\n",
      "    funny how people lose it under pressure....)\n",
      "  back\n",
      "In the attire proper\n",
      "  to his Royal State.\n",
      "\n",
      "His ass is covered---if Herod has no problem,\n",
      "Caesar certainly won't.  The fool can be king\n",
      "of whatever world he wants\n",
      "as long as it's not Caesar's.\n",
      "\n",
      "``I'm letting him go,'' he said with a shout.\n",
      "(Looks like he'll last this one out....)\n",
      "\n",
      "The crowd's reaction puzzled him.\n",
      "  They really wanted him dead.\n",
      "They didn't want the King of the Jews,\n",
      "  They wanted Barabbas instead\n",
      "(And, as Josephus records, they got him)\n",
      "\n",
      "Oh well, he thought,\n",
      "They all look the same to me.\n",
      "And we'll get Barabbas next time.\n",
      "\n",
      "And if I can get them to say\n",
      "   ``We have no king but Caesar!''\n",
      "   By killing a madman,\n",
      "Hell, I'll kill ten a day.\n",
      "\n",
      "And then Pilate had his fun\n",
      "  A little joke\n",
      "    Short\n",
      "      To the point\n",
      "        Trilingual\n",
      "\n",
      "And all this\n",
      "Went as it always does\n",
      "When someone gets caught\n",
      "In the gears of government\n",
      "\n",
      "And there's a scientific explanation\n",
      "     (no doubt)\n",
      "For the superstitious rumors\n",
      "     (persisting to this day)\n",
      "That it didn't all end\n",
      "With a tomb\n",
      "and a Roman squadron on guard.\n",
      "\n",
      "Our sophomore doesn't know about this\n",
      "He doesn't recognize his kindred spirit\n",
      "(Or truth either, as he admits).\n",
      "\n",
      "I guess we haven't learned much\n",
      "in two thousand years.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "--\n",
      "-Fred Gilham    gilham@csl.sri.com\n",
      "\"Peace is only better than war when it's not hell too.  War being hell\n",
      "makes sense.\"\n",
      "               -Walker Percy, THE SECOND COMING\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# # SOLUTION (yes, we are cool)\n",
    "print('taille du jeu de données: \\n \\t {} posts de forum en total'.format(len(x_train) + len(x_test)))\n",
    "print('\\t {} posts pour le training'.format(len(x_train)))\n",
    "for i in range(len(categories)):\n",
    "    num = sum(y_train == i)\n",
    "    print(\"\\t\\t {} {}\".format(num, categories[i]))\n",
    "    print('\\t {} posts pour le test'.format(len(x_test)))\n",
    "for i in range(len(categories)):\n",
    "    num = sum(y_test == i)\n",
    "    print(\"\\t\\t {} {}\".format(num, categories[i]))\n",
    "\n",
    "print('\\n')\n",
    "print('EXEMPLE: \\n')\n",
    "print(x_train[0])\n",
    "\n",
    "\n",
    "def clean_post(post: str, remove_start: tuple):\n",
    "    clean_lines = []\n",
    "    for line in post.splitlines():\n",
    "        if not line.startswith(remove_start):\n",
    "            clean_lines.append(line)\n",
    "    return '\\n'.join(clean_lines)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# SOLUTION (yes, again, we are cool)\n",
    "remove_start = (\n",
    "    'From:',\n",
    "    'Subject:',\n",
    "    'Reply-To:',\n",
    "    'In-Reply-To:',\n",
    "    'Nntp-Posting-Host:',\n",
    "    'Organization:',\n",
    "    'X-Mailer:',\n",
    "    'In article <',\n",
    "    'Lines:',\n",
    "    'NNTP-Posting-Host:',\n",
    "    'Summary:',\n",
    "    'Article-I.D.:'\n",
    ")\n",
    "x_train = [clean_post(p, remove_start) for p in x_train]\n",
    "x_test = [clean_post(p, remove_start) for p in x_test]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (744 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [101, 1996, 13758, 1006, 10900, 1015, 1024, 2570, 1007, 1996, 13758, 2758, 1010, 1036, 1036, 2054, 2003, 3606, 1029, 1005, 1005, 1998, 4332, 2000, 19021, 2243, 1999, 1996, 17005, 1997, 2010, 12746, 1012, 2129, 2715, 2129, 15236, 2129, 5622, 5677, 5844, 2129, 2715, 2129, 15236, 2129, 5622, 5677, 5844, 2027, 16883, 1996, 13758, 1010, 2108, 2137, 2987, 1005, 1056, 2113, 2008, 2010, 1036, 1036, 3160, 1005, 1005, 2715, 18386, 26881, 2001, 2356, 2077, 1010, 2011, 1037, 2715, 18386, 26881, 3923, 2063, 24686, 3761, 1006, 2805, 2025, 2700, 1007, 2040, 3047, 2000, 2444, 2048, 4595, 2086, 3283, 1012, 2066, 2116, 8801, 2002, 8725, 2625, 2055, 15084, 2084, 3463, 2625, 2055, 4515, 2084, 2965, 2625, 2055, 2505, 2084, 4363, 2010, 3105, 1006, 1998, 2010, 2132, 1007, 1012, 2057, 2453, 2655, 2032, 1037, 2978, 12077, 2295, 1036, 3813, 1005, 2052, 2022, 2785, 2121, 1006, 1998, 2053, 4797, 13125, 1010, 2040, 2292, 6343, 2175, 1010, 4191, 2012, 2010, 27327, 2791, 1007, 2002, 2134, 1005, 1056, 2066, 2010, 3105, 1025, 3383, 2002, 2053, 2936, 5113, 2005, 2488, 1006, 4496, 8615, 4788, 1010, 3272, 4953, 2010, 2132, 1007, 1012, 1998, 2043, 2122, 19863, 2100, 5181, 2007, 2037, 4641, 1011, 1045, 1011, 2663, 1010, 17448, 1011, 2017, 1011, 4558, 9530, 8630, 6824, 2015, 2716, 5743, 2037, 28441, 1010, 2010, 2034, 14982, 2001, 2000, 2377, 1996, 3142, 1024, 1036, 1036, 1045, 2424, 2498, 3308, 2007, 2032, 1010, 2156, 2000, 2009, 25035, 1012, 1005, 1005, 2021, 2043, 2027, 3855, 1036, 2332, 1005, 1998, 1036, 11604, 1005, 2010, 2540, 10619, 1012, 2065, 2002, 2730, 2037, 28441, 2002, 1005, 1040, 2707, 1037, 11421, 1998, 4558, 2010, 3105, 1006, 1998, 2010, 2132, 1007, 2065, 2002, 5552, 1996, 2332, 1997, 1996, 5181, 2002, 1005, 1040, 18138, 2125, 11604, 1998, 4558, 2010, 3105, 1006, 1998, 2010, 2132, 1007, 1998, 2043, 2010, 2564, 2409, 2032, 2000, 2031, 2498, 2000, 2079, 2007, 1996, 19556, 10223, 2102, 2016, 2134, 1005, 1056, 2425, 2032, 2505, 2002, 2910, 1005, 1056, 2525, 6618, 2041, 1012, 2061, 2002, 18975, 2098, 1012, 1036, 1036, 2025, 2026, 7360, 999, 2202, 2032, 2000, 2156, 5394, 2094, 999, 1005, 1005, 1006, 2040, 2074, 3047, 2000, 2022, 1999, 2237, 1012, 1012, 1012, 1012, 1007, 5394, 2094, 12315, 1996, 14571, 2021, 2347, 1005, 1056, 5191, 1998, 2741, 1996, 4629, 1011, 4416, 2094, 7966, 1006, 2040, 3402, 2134, 1005, 1056, 2031, 2172, 2000, 2360, 1010, 6057, 2129, 2111, 4558, 2009, 2104, 3778, 1012, 1012, 1012, 1012, 1007, 2067, 1999, 1996, 20426, 5372, 2000, 2010, 2548, 2110, 1012, 2010, 4632, 2003, 3139, 1011, 1011, 1011, 2065, 5394, 2094, 2038, 2053, 3291, 1010, 11604, 5121, 2180, 1005, 1056, 1012, 1996, 7966, 2064, 2022, 2332, 1997, 3649, 2088, 2002, 4122, 2004, 2146, 2004, 2009, 1005, 1055, 2025, 11604, 1005, 1055, 1012, 1036, 1036, 1045, 1005, 1049, 5599, 2032, 2175, 1010, 1005, 1005, 2002, 2056, 2007, 1037, 11245, 1012, 1006, 3504, 2066, 2002, 1005, 2222, 2197, 2023, 2028, 2041, 1012, 1012, 1012, 1012, 1007, 1996, 4306, 1005, 1055, 4668, 14909, 2032, 1012, 2027, 2428, 2359, 2032, 2757, 1012, 2027, 2134, 1005, 1056, 2215, 1996, 2332, 1997, 1996, 5181, 1010, 2027, 2359, 3347, 7875, 22083, 2612, 1006, 1998, 1010, 2004, 3312, 2271, 2636, 1010, 2027, 2288, 2032, 1007, 2821, 2092, 1010, 2002, 2245, 1010, 2027, 2035, 2298, 1996, 2168, 2000, 2033, 1012, 1998, 2057, 1005, 2222, 2131, 3347, 7875, 22083, 2279, 2051, 1012, 1998, 2065, 1045, 2064, 2131, 2068, 2000, 2360, 1036, 1036, 2057, 2031, 2053, 2332, 2021, 11604, 999, 1005, 1005, 2011, 4288, 1037, 28441, 1010, 3109, 1010, 1045, 1005, 2222, 3102, 2702, 1037, 2154, 1012, 1998, 2059, 14255, 13806, 2018, 2010, 4569, 1037, 2210, 8257, 2460, 2000, 1996, 2391, 13012, 2989, 8787, 1998, 2035, 2023, 2253, 2004, 2009, 2467, 2515, 2043, 2619, 4152, 3236, 1999, 1996, 19456, 1997, 2231, 1998, 2045, 1005, 1055, 1037, 4045, 7526, 1006, 2053, 4797, 1007, 2005, 1996, 3565, 16643, 20771, 11256, 1006, 29486, 2075, 2000, 2023, 2154, 1007, 2008, 2009, 2134, 1005, 1056, 2035, 2203, 2007, 1037, 8136, 1998, 1037, 3142, 3704, 2006, 3457, 1012, 2256, 13758, 2987, 1005, 1056, 2113, 2055, 2023, 2002, 2987, 1005, 1056, 6807, 2010, 2785, 5596, 4382, 1006, 2030, 3606, 2593, 1010, 2004, 2002, 14456, 1007, 1012, 1045, 3984, 2057, 4033, 1005, 1056, 4342, 2172, 1999, 2048, 4595, 2086, 1012, 1011, 1011, 1011, 5965, 13097, 3511, 13097, 3511, 1030, 20116, 2140, 1012, 5185, 1012, 4012, 1000, 3521, 2003, 2069, 2488, 2084, 2162, 2043, 2009, 1005, 1055, 2025, 3109, 2205, 1012, 2162, 2108, 3109, 3084, 3168, 1012, 1000, 1011, 5232, 11312, 1010, 1996, 2117, 2746, 102], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n",
      "['[CLS]', 'the', 'sophomore', '(', 'romans', '1', ':', '22', ')', 'the', 'sophomore', 'says', ',', '`', '`', 'what', 'is', 'truth', '?', \"'\", \"'\", 'and', 'turns', 'to', 'bas', '##k', 'in', 'the', 'admiration', 'of', 'his', 'peers', '.', 'how', 'modern', 'how', 'daring', 'how', 'li', '##ber', '##ating', 'how', 'modern', 'how', 'daring', 'how', 'li', '##ber', '##ating', 'they', 'chant', 'the', 'sophomore', ',', 'being', 'american', 'doesn', \"'\", 't', 'know', 'that', 'his', '`', '`', 'question', \"'\", \"'\", 'modern', 'skeptical', 'cynical', 'was', 'asked', 'before', ',', 'by', 'a', 'modern', 'skeptical', 'cynical', 'urban', '##e', 'cosmopolitan', 'politician', '(', 'appointed', 'not', 'elected', ')', 'who', 'happened', 'to', 'live', 'two', 'thousand', 'years', 'ago', '.', 'like', 'many', 'politicians', 'he', 'cared', 'less', 'about', 'ideals', 'than', 'results', 'less', 'about', 'ends', 'than', 'means', 'less', 'about', 'anything', 'than', 'keeping', 'his', 'job', '(', 'and', 'his', 'head', ')', '.', 'we', 'might', 'call', 'him', 'a', 'bit', 'brutal', 'though', '`', 'firm', \"'\", 'would', 'be', 'kind', '##er', '(', 'and', 'no', 'doubt', 'stalin', ',', 'who', 'let', 'nobody', 'go', ',', 'laughed', 'at', 'his', 'lax', '##ness', ')', 'he', 'didn', \"'\", 't', 'like', 'his', 'job', ';', 'perhaps', 'he', 'no', 'longer', 'hoped', 'for', 'better', '(', 'nor', 'feared', 'worse', ',', 'except', 'regarding', 'his', 'head', ')', '.', 'and', 'when', 'these', 'wil', '##y', 'jews', 'with', 'their', 'heads', '-', 'i', '-', 'win', ',', 'tails', '-', 'you', '-', 'lose', 'con', '##und', '##rum', '##s', 'brought', 'forth', 'their', 'madman', ',', 'his', 'first', 'impulse', 'was', 'to', 'play', 'the', 'roman', ':', '`', '`', 'i', 'find', 'nothing', 'wrong', 'with', 'him', ',', 'see', 'to', 'it', 'yourselves', '.', \"'\", \"'\", 'but', 'when', 'they', 'mentioned', '`', 'king', \"'\", 'and', '`', 'caesar', \"'\", 'his', 'heart', 'froze', '.', 'if', 'he', 'killed', 'their', 'madman', 'he', \"'\", 'd', 'start', 'a', 'riot', 'and', 'lose', 'his', 'job', '(', 'and', 'his', 'head', ')', 'if', 'he', 'saved', 'the', 'king', 'of', 'the', 'jews', 'he', \"'\", 'd', 'piss', 'off', 'caesar', 'and', 'lose', 'his', 'job', '(', 'and', 'his', 'head', ')', 'and', 'when', 'his', 'wife', 'told', 'him', 'to', 'have', 'nothing', 'to', 'do', 'with', 'the', 'righteous', 'lou', '##t', 'she', 'didn', \"'\", 't', 'tell', 'him', 'anything', 'he', 'hadn', \"'\", 't', 'already', 'figured', 'out', '.', 'so', 'he', 'punt', '##ed', '.', '`', '`', 'not', 'my', 'jurisdiction', '!', 'take', 'him', 'to', 'see', 'hero', '##d', '!', \"'\", \"'\", '(', 'who', 'just', 'happened', 'to', 'be', 'in', 'town', '.', '.', '.', '.', ')', 'hero', '##d', 'appreciated', 'the', 'courtesy', 'but', 'wasn', \"'\", 't', 'worried', 'and', 'sent', 'the', 'sharp', '-', 'tongue', '##d', 'fool', '(', 'who', 'suddenly', 'didn', \"'\", 't', 'have', 'much', 'to', 'say', ',', 'funny', 'how', 'people', 'lose', 'it', 'under', 'pressure', '.', '.', '.', '.', ')', 'back', 'in', 'the', 'attire', 'proper', 'to', 'his', 'royal', 'state', '.', 'his', 'ass', 'is', 'covered', '-', '-', '-', 'if', 'hero', '##d', 'has', 'no', 'problem', ',', 'caesar', 'certainly', 'won', \"'\", 't', '.', 'the', 'fool', 'can', 'be', 'king', 'of', 'whatever', 'world', 'he', 'wants', 'as', 'long', 'as', 'it', \"'\", 's', 'not', 'caesar', \"'\", 's', '.', '`', '`', 'i', \"'\", 'm', 'letting', 'him', 'go', ',', \"'\", \"'\", 'he', 'said', 'with', 'a', 'shout', '.', '(', 'looks', 'like', 'he', \"'\", 'll', 'last', 'this', 'one', 'out', '.', '.', '.', '.', ')', 'the', 'crowd', \"'\", 's', 'reaction', 'puzzled', 'him', '.', 'they', 'really', 'wanted', 'him', 'dead', '.', 'they', 'didn', \"'\", 't', 'want', 'the', 'king', 'of', 'the', 'jews', ',', 'they', 'wanted', 'bar', '##ab', '##bas', 'instead', '(', 'and', ',', 'as', 'joseph', '##us', 'records', ',', 'they', 'got', 'him', ')', 'oh', 'well', ',', 'he', 'thought', ',', 'they', 'all', 'look', 'the', 'same', 'to', 'me', '.', 'and', 'we', \"'\", 'll', 'get', 'bar', '##ab', '##bas', 'next', 'time', '.', 'and', 'if', 'i', 'can', 'get', 'them', 'to', 'say', '`', '`', 'we', 'have', 'no', 'king', 'but', 'caesar', '!', \"'\", \"'\", 'by', 'killing', 'a', 'madman', ',', 'hell', ',', 'i', \"'\", 'll', 'kill', 'ten', 'a', 'day', '.', 'and', 'then', 'pi', '##late', 'had', 'his', 'fun', 'a', 'little', 'joke', 'short', 'to', 'the', 'point', 'tri', '##ling', '##ual', 'and', 'all', 'this', 'went', 'as', 'it', 'always', 'does', 'when', 'someone', 'gets', 'caught', 'in', 'the', 'gears', 'of', 'government', 'and', 'there', \"'\", 's', 'a', 'scientific', 'explanation', '(', 'no', 'doubt', ')', 'for', 'the', 'super', '##sti', '##tious', 'rumors', '(', 'persist', '##ing', 'to', 'this', 'day', ')', 'that', 'it', 'didn', \"'\", 't', 'all', 'end', 'with', 'a', 'tomb', 'and', 'a', 'roman', 'squadron', 'on', 'guard', '.', 'our', 'sophomore', 'doesn', \"'\", 't', 'know', 'about', 'this', 'he', 'doesn', \"'\", 't', 'recognize', 'his', 'kind', '##red', 'spirit', '(', 'or', 'truth', 'either', ',', 'as', 'he', 'admits', ')', '.', 'i', 'guess', 'we', 'haven', \"'\", 't', 'learned', 'much', 'in', 'two', 'thousand', 'years', '.', '-', '-', '-', 'fred', 'gil', '##ham', 'gil', '##ham', '@', 'cs', '##l', '.', 'sri', '.', 'com', '\"', 'peace', 'is', 'only', 'better', 'than', 'war', 'when', 'it', \"'\", 's', 'not', 'hell', 'too', '.', 'war', 'being', 'hell', 'makes', 'sense', '.', '\"', '-', 'walker', 'percy', ',', 'the', 'second', 'coming', '[SEP]'] \n",
      "\n",
      "[CLS] the sophomore ( romans 1 : 22 ) the sophomore says, ` ` what is truth?'' and turns to bask in the admiration of his peers. how modern how daring how liberating how modern how daring how liberating they chant the sophomore, being american doesn't know that his ` ` question'' modern skeptical cynical was asked before, by a modern skeptical cynical urbane cosmopolitan politician ( appointed not elected ) who happened to live two thousand years ago. like many politicians he cared less about ideals than results less about ends than means less about anything than keeping his job ( and his head ). we might call him a bit brutal though ` firm'would be kinder ( and no doubt stalin, who let nobody go, laughed at his laxness ) he didn't like his job ; perhaps he no longer hoped for better ( nor feared worse, except regarding his head ). and when these wily jews with their heads - i - win, tails - you - lose conundrums brought forth their madman, his first impulse was to play the roman : ` ` i find nothing wrong with him, see to it yourselves.'' but when they mentioned ` king'and ` caesar'his heart froze. if he killed their madman he'd start a riot and lose his job ( and his head ) if he saved the king of the jews he'd piss off caesar and lose his job ( and his head ) and when his wife told him to have nothing to do with the righteous lout she didn't tell him anything he hadn't already figured out. so he punted. ` ` not my jurisdiction! take him to see herod!'' ( who just happened to be in town.... ) herod appreciated the courtesy but wasn't worried and sent the sharp - tongued fool ( who suddenly didn't have much to say, funny how people lose it under pressure.... ) back in the attire proper to his royal state. his ass is covered - - - if herod has no problem, caesar certainly won't. the fool can be king of whatever world he wants as long as it's not caesar's. ` ` i'm letting him go,'' he said with a shout. ( looks like he'll last this one out.... ) the crowd's reaction puzzled him. they really wanted him dead. they didn't want the king of the jews, they wanted barabbas instead ( and, as josephus records, they got him ) oh well, he thought, they all look the same to me. and we'll get barabbas next time. and if i can get them to say ` ` we have no king but caesar!'' by killing a madman, hell, i'll kill ten a day. and then pilate had his fun a little joke short to the point trilingual and all this went as it always does when someone gets caught in the gears of government and there's a scientific explanation ( no doubt ) for the superstitious rumors ( persisting to this day ) that it didn't all end with a tomb and a roman squadron on guard. our sophomore doesn't know about this he doesn't recognize his kindred spirit ( or truth either, as he admits ). i guess we haven't learned much in two thousand years. - - - fred gilham gilham @ csl. sri. com \" peace is only better than war when it's not hell too. war being hell makes sense. \" - walker percy, the second coming [SEP] \n",
      "\n",
      "---------------- \n",
      "\n",
      "{'input_ids': [101, 1028, 1045, 2001, 5327, 2008, 1011, 1011, 2174, 1996, 3663, 2001, 10395, 1011, 1011, 1996, 1028, 3200, 2052, 3961, 10109, 1010, 2061, 1996, 18079, 1005, 1056, 2071, 5271, 2009, 2000, 2393, 3477, 2005, 1996, 1028, 5606, 1997, 5190, 1997, 6363, 1997, 11727, 22667, 2383, 2000, 3336, 28032, 1028, 1047, 20409, 2100, 12849, 21898, 1004, 2010, 19311, 1997, 8351, 1012, 1028, 1999, 2070, 2148, 2137, 3032, 1010, 2044, 2576, 6151, 2229, 7895, 13510, 5419, 1010, 1996, 2155, 2052, 2131, 1037, 5060, 1997, 2331, 1998, 1037, 3021, 2005, 1996, 13148, 1997, 1996, 2303, 1012, 2017, 4593, 2228, 2008, 2052, 2022, 1037, 2204, 2801, 1012, 1996, 2976, 2231, 7531, 2023, 2895, 2114, 12849, 21898, 1998, 2010, 8771, 1010, 5129, 2068, 2005, 4868, 2420, 1010, 5117, 1999, 8317, 8309, 1010, 2109, 3082, 2510, 3941, 2114, 2149, 4480, 2006, 2149, 5800, 1025, 1998, 2085, 2008, 1996, 7328, 3236, 2543, 2096, 2027, 2020, 14107, 1999, 20116, 3806, 2044, 10591, 8198, 1999, 1996, 2311, 1025, 4487, 3736, 6767, 9333, 2035, 5368, 1012, 2502, 2567, 2003, 2025, 2467, 2157, 1012, 1008, 1008, 1008, 2703, 4388, 2358, 7140, 18142, 2102, 1008, 1008, 1008, 3996, 10133, 2966, 2415, 1008, 1008, 1008, 4274, 1024, 21877, 2015, 2509, 1030, 12731, 3490, 2595, 2546, 1012, 10507, 1012, 3996, 1012, 3968, 2226, 1008, 1008, 1008, 2035, 10740, 2024, 2026, 2219, 102], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n",
      "['[CLS]', '>', 'i', 'was', 'hoping', 'that', '-', '-', 'however', 'the', 'situation', 'was', 'resolved', '-', '-', 'the', '>', 'property', 'would', 'remain', 'intact', ',', 'so', 'the', 'gov', \"'\", 't', 'could', 'sell', 'it', 'to', 'help', 'pay', 'for', 'the', '>', 'hundreds', 'of', 'thousands', 'of', 'dollars', 'of', 'expenses', 'incurred', 'having', 'to', 'baby', '##sit', '>', 'k', '##raz', '##y', 'ko', '##resh', '&', 'his', 'flock', 'of', 'sheep', '.', '>', 'in', 'some', 'south', 'american', 'countries', ',', 'after', 'political', 'und', '##es', '##ira', '##bles', 'disappeared', ',', 'the', 'family', 'would', 'get', 'a', 'notice', 'of', 'death', 'and', 'a', 'bill', 'for', 'the', 'disposal', 'of', 'the', 'body', '.', 'you', 'apparently', 'think', 'that', 'would', 'be', 'a', 'good', 'idea', '.', 'the', 'federal', 'government', 'initiated', 'this', 'action', 'against', 'ko', '##resh', 'and', 'his', 'followers', ',', 'surrounded', 'them', 'for', '51', 'days', ',', 'engaged', 'in', 'psychological', 'warfare', ',', 'used', 'heavy', 'military', 'equipment', 'against', 'us', 'citizens', 'on', 'us', 'soil', ';', 'and', 'now', 'that', 'the', 'compound', 'caught', 'fire', 'while', 'they', 'were', 'pumping', 'in', 'cs', 'gas', 'after', 'knocking', 'holes', 'in', 'the', 'building', ';', 'di', '##sa', '##vo', '##ws', 'all', 'responsibility', '.', 'big', 'brother', 'is', 'not', 'always', 'right', '.', '*', '*', '*', 'paul', 'eric', 'st', '##ou', '##ffle', '##t', '*', '*', '*', 'columbia', 'presbyterian', 'medical', 'center', '*', '*', '*', 'internet', ':', 'pe', '##s', '##3', '@', 'cu', '##ni', '##x', '##f', '.', 'cc', '.', 'columbia', '.', 'ed', '##u', '*', '*', '*', 'all', 'opinions', 'are', 'my', 'own', '[SEP]'] \n",
      "\n",
      "[CLS] > i was hoping that - - however the situation was resolved - - the > property would remain intact, so the gov't could sell it to help pay for the > hundreds of thousands of dollars of expenses incurred having to babysit > krazy koresh & his flock of sheep. > in some south american countries, after political undesirables disappeared, the family would get a notice of death and a bill for the disposal of the body. you apparently think that would be a good idea. the federal government initiated this action against koresh and his followers, surrounded them for 51 days, engaged in psychological warfare, used heavy military equipment against us citizens on us soil ; and now that the compound caught fire while they were pumping in cs gas after knocking holes in the building ; disavows all responsibility. big brother is not always right. * * * paul eric stoufflet * * * columbia presbyterian medical center * * * internet : pes3 @ cunixf. cc. columbia. edu * * * all opinions are my own [SEP] \n",
      "\n",
      "---------------- \n",
      "\n",
      "{'input_ids': [101, 1999, 2831, 1012, 4331, 1012, 4409, 1010, 14855, 5620, 2102, 15136, 1009, 1030, 15091, 1012, 3968, 2226, 1006, 6498, 1037, 7977, 2386, 1007, 7009, 1024, 2092, 6498, 1045, 5993, 2007, 2017, 2000, 2070, 4847, 1012, 1012, 1012, 2625, 2115, 11379, 10697, 1012, 1996, 18079, 1005, 24098, 2102, 2467, 2442, 2663, 999, 2130, 2065, 2027, 3102, 2296, 2158, 2308, 1998, 2775, 1012, 1012, 1012, 1012, 2011, 2643, 2027, 2442, 2663, 2012, 2035, 5366, 1012, 1012, 1012, 1012, 1012, 1012, 2023, 6433, 2058, 1998, 2058, 1998, 2058, 1999, 2023, 2406, 1012, 11082, 2191, 21917, 1010, 2131, 1996, 22692, 2811, 2000, 3104, 2039, 2673, 1010, 2292, 1996, 4584, 2202, 1996, 3684, 2005, 2327, 2968, 28072, 4385, 1012, 1012, 1012, 4385, 1012, 1012, 1012, 1028, 1045, 2572, 5305, 2007, 24665, 7416, 2546, 2005, 1996, 2972, 2092, 2108, 1997, 2023, 3842, 1998, 1996, 1028, 4552, 1999, 4447, 2000, 4047, 1012, 1028, 1028, 2101, 1010, 1028, 6498, 1028, 1028, 2101, 22294, 2100, 102], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n",
      "['[CLS]', 'in', 'talk', '.', 'politics', '.', 'guns', ',', 'ja', '##gs', '##t', '##18', '+', '@', 'pitt', '.', 'ed', '##u', '(', 'josh', 'a', 'gross', '##man', ')', 'writes', ':', 'well', 'josh', 'i', 'agree', 'with', 'you', 'to', 'some', 'respect', '.', '.', '.', 'less', 'your', 'spelling', 'errors', '.', 'the', 'gov', \"'\", 'mn', '##t', 'always', 'must', 'win', '!', 'even', 'if', 'they', 'kill', 'every', 'man', 'women', 'and', 'child', '.', '.', '.', '.', 'by', 'god', 'they', 'must', 'win', 'at', 'all', 'costs', '.', '.', '.', '.', '.', '.', 'this', 'happens', 'over', 'and', 'over', 'and', 'over', 'in', 'this', 'country', '.', 'lets', 'make', 'excuses', ',', 'get', 'the', 'worthless', 'press', 'to', 'cover', 'up', 'everything', ',', 'let', 'the', 'officials', 'take', 'the', 'heat', 'for', 'top', 'management', 'stupidity', 'etc', '.', '.', '.', 'etc', '.', '.', '.', '>', 'i', 'am', 'sick', 'with', 'gr', '##ei', '##f', 'for', 'the', 'entire', 'well', 'being', 'of', 'this', 'nation', 'and', 'the', '>', 'constitution', 'in', 'claims', 'to', 'protect', '.', '>', '>', 'later', ',', '>', 'josh', '>', '>', 'later', 'mort', '##y', '[SEP]'] \n",
      "\n",
      "[CLS] in talk. politics. guns, jagst18 + @ pitt. edu ( josh a grossman ) writes : well josh i agree with you to some respect... less your spelling errors. the gov'mnt always must win! even if they kill every man women and child.... by god they must win at all costs...... this happens over and over and over in this country. lets make excuses, get the worthless press to cover up everything, let the officials take the heat for top management stupidity etc... etc... > i am sick with greif for the entire well being of this nation and the > constitution in claims to protect. > > later, > josh > > later morty [SEP] \n",
      "\n",
      "---------------- \n",
      "\n"
     ]
    }
   ],
   "source": [
    "## TOKENISATION\n",
    "\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import DistilBertTokenizer\n",
    "from transformers import DistilBertTokenizerFast\n",
    "\n",
    "MAX_LEN = 512\n",
    "\n",
    "tokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-uncased', padding=True, truncation=True)\n",
    "\n",
    "# let's check out how the tokenizer works\n",
    "for n in range(3):\n",
    "    # tokenize forum post\n",
    "    tokenizer_out = tokenizer(x_train[n])\n",
    "    # convert numerical tokens to alphabetical tokens\n",
    "    encoded_tok = tokenizer.convert_ids_to_tokens(tokenizer_out.input_ids)\n",
    "    # decode tokens back to string\n",
    "    decoded = tokenizer.decode(tokenizer_out.input_ids)\n",
    "    print(tokenizer_out)\n",
    "    print(encoded_tok, '\\n')\n",
    "    print(decoded, '\\n')\n",
    "    print('---------------- \\n')\n",
    "\n",
    "\n",
    "class PostsDataset(Dataset):\n",
    "    def __init__(self, posts, labels, tokenizer, max_len):\n",
    "        # variables that are set when the class is instantiated\n",
    "        self.posts = posts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.posts)\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        # select the post and its category\n",
    "        post = str(self.posts[item])\n",
    "        label = self.labels[item]\n",
    "        # tokenize the post\n",
    "        tokenizer_out = self.tokenizer(\n",
    "            post,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_len,\n",
    "            return_token_type_ids=False,\n",
    "            pad_to_max_length=True,\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt',\n",
    "            truncation=True\n",
    "        )\n",
    "        # return a dictionary with the output of the tokenizer and the label\n",
    "        return  {\n",
    "            'input_ids': tokenizer_out['input_ids'].flatten(),\n",
    "            'attention_mask': tokenizer_out['attention_mask'].flatten(),\n",
    "            'label': torch.tensor(label, dtype=torch.long)\n",
    "        }\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# instantiate two PostsDatasets\n",
    "train_dataset = PostsDataset(x_train, y_train, tokenizer, MAX_LEN)\n",
    "test_dataset = PostsDataset(x_test, y_test, tokenizer, MAX_LEN)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_layer_norm.bias', 'vocab_transform.bias', 'vocab_transform.weight', 'vocab_layer_norm.weight', 'vocab_projector.bias', 'vocab_projector.weight']\n",
      "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "D:\\anaconda\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2221: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 512, 768])\n",
      "DistilBertConfig {\n",
      "  \"_name_or_path\": \"distilbert-base-uncased\",\n",
      "  \"activation\": \"gelu\",\n",
      "  \"architectures\": [\n",
      "    \"DistilBertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"dim\": 768,\n",
      "  \"dropout\": 0.1,\n",
      "  \"hidden_dim\": 3072,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"distilbert\",\n",
      "  \"n_heads\": 12,\n",
      "  \"n_layers\": 6,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"qa_dropout\": 0.1,\n",
      "  \"seq_classif_dropout\": 0.2,\n",
      "  \"sinusoidal_pos_embds\": false,\n",
      "  \"tie_weights_\": true,\n",
      "  \"transformers_version\": \"4.14.1\",\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## MODEL\n",
    "\n",
    "from transformers import DistilBertModel\n",
    "\n",
    "from transformers import DistilBertPreTrainedModel, DistilBertConfig\n",
    "\n",
    "PRE_TRAINED_MODEL_NAME = 'distilbert-base-uncased'\n",
    "\n",
    "\n",
    "distilbert = DistilBertModel.from_pretrained(PRE_TRAINED_MODEL_NAME)\n",
    "\n",
    "first_post = train_dataset[0]\n",
    "\n",
    "hidden_state = distilbert(\n",
    "    input_ids=first_post['input_ids'].unsqueeze(0), attention_mask=first_post['attention_mask'].unsqueeze(0)\n",
    ")\n",
    "\n",
    "print(hidden_state[0].shape)\n",
    "\n",
    "print(distilbert.config)\n",
    "\n",
    "\n",
    "\n",
    "class Pre_Classifier(torch.nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.pre_classifier = torch.nn.Linear(config.dim, config.dim)\n",
    "        self.activation = torch.nn.Tanh()\n",
    "\n",
    "    def forward(self, hidden_states):\n",
    "        first_token_tensor = hidden_states[:, 0]\n",
    "        pooled_output = self.pre_classifier(first_token_tensor)\n",
    "        pooled_output = self.activation(pooled_output)\n",
    "        return pooled_output\n",
    "\n",
    "class DistilBertForPostClassification(DistilBertPreTrainedModel):\n",
    "    def __init__(self, config, num_labels, freeze_encoder=False):\n",
    "        # instantiate the parent class DistilBertPreTrainedModel\n",
    "        super().__init__(config)\n",
    "        # instantiate num. of classes\n",
    "        self.num_labels = num_labels\n",
    "        # instantiate and load a pretrained DistilBERT model as encoder\n",
    "        self.encoder = DistilBertModel.from_pretrained(PRE_TRAINED_MODEL_NAME)\n",
    "        # freeze the encoder parameters if required (Q1)\n",
    "        if freeze_encoder:\n",
    "            for param in self.encoder.parameters():\n",
    "                param.requires_grad = False\n",
    "        #Adding a pre classifier to improve the classifier efficiency\n",
    "        self.pre_classifier = Pre_Classifier(config)\n",
    "        # the classifier: a feed-forward layer attached to the encoder's head\n",
    "        \n",
    "        self.classifier = torch.nn.Linear(in_features=config.dim, out_features=self.num_labels, bias=True)\n",
    "        \n",
    "        # instantiate a dropout function for the classifier's input\n",
    "        self.dropout = torch.nn.Dropout(p=0.3)\n",
    "\n",
    "\n",
    "    def forward(\n",
    "            self,\n",
    "            input_ids=None,\n",
    "            attention_mask=None,\n",
    "            head_mask=None,\n",
    "            inputs_embeds=None,\n",
    "            labels=None,\n",
    "            output_attentions=None,\n",
    "            output_hidden_states=None,\n",
    "    ):\n",
    "        # encode a batch of sequences with DistilBERT\n",
    "        encoder_output = self.encoder(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            head_mask=head_mask,\n",
    "            inputs_embeds=inputs_embeds,\n",
    "            output_attentions=output_attentions,\n",
    "            output_hidden_states=output_hidden_states,\n",
    "        )\n",
    "        # extract the hidden representations from the encoder output\n",
    "        hidden_state = encoder_output[0]  # (bs, seq_len, dim)\n",
    "        # only select the encoding corresponding to the first token\n",
    "        # of each sequence in the batch (Q2)\n",
    "        pooled_output = hidden_state[:, 0]  # (bs, dim)\n",
    "\n",
    "        pooled_output = self.pre_classifier(hidden_state)\n",
    "        # apply dropout\n",
    "        pooled_output = self.dropout(pooled_output)  # (bs, dim)\n",
    "\n",
    "        # feed into the classifier\n",
    "        logits = self.classifier(pooled_output)  # (bs, dim)\n",
    "\n",
    "        outputs = (logits,) + encoder_output[1:]\n",
    "\n",
    "\n",
    "\n",
    "        if labels is not None: # (Q3)\n",
    "            # instantiate loss function\n",
    "            loss_fct = torch.nn.CrossEntropyLoss()\n",
    "            # calculate loss\n",
    "            loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n",
    "            # aggregate outputs\n",
    "            outputs = (loss,) + outputs\n",
    "\n",
    "        return outputs  # (loss), logits, (hidden_states), (attentions)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_layer_norm.bias', 'vocab_transform.bias', 'vocab_transform.weight', 'vocab_layer_norm.weight', 'vocab_projector.bias', 'vocab_projector.weight']\n",
      "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "***** Running training *****\n",
      "  Num examples = 2332\n",
      "  Num Epochs = 4\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 1168\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1168' max='1168' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1168/1168 01:10, Epoch 4/4]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.285800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>1.288100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>1.104600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>0.922300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.771700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>0.698100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.578000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>0.526400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.431800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>0.440500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.371200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>550</td>\n",
       "      <td>0.369900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.345200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>650</td>\n",
       "      <td>0.323400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>0.294500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>750</td>\n",
       "      <td>0.276400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.306400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>850</td>\n",
       "      <td>0.259200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>0.255300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>950</td>\n",
       "      <td>0.256400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.283600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1050</td>\n",
       "      <td>0.235100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>0.240900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1150</td>\n",
       "      <td>0.266600</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./results\\checkpoint-500\n",
      "Configuration saved in ./results\\checkpoint-500\\config.json\n",
      "Model weights saved in ./results\\checkpoint-500\\pytorch_model.bin\n",
      "D:\\anaconda\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2221: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "Saving model checkpoint to ./results\\checkpoint-1000\n",
      "Configuration saved in ./results\\checkpoint-1000\\config.json\n",
      "Model weights saved in ./results\\checkpoint-1000\\pytorch_model.bin\n",
      "D:\\anaconda\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2221: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "***** Running Prediction *****\n",
      "  Num examples = 1553\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='195' max='195' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [195/195 00:10]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./my_model\n",
      "Configuration saved in ./my_model\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions: \n",
      " [[ 6.0417085  -0.94372606 -2.5727444  -1.865699  ]\n",
      " [ 3.8881085  -0.22453788 -1.9770339  -1.1293151 ]\n",
      " [ 1.6429119  -0.21760496 -0.7635118  -0.3701588 ]\n",
      " ...\n",
      " [-0.32982612  0.29814097 -0.48656666  0.7520254 ]\n",
      " [-0.2223302   0.0083893  -1.2901748   1.7011836 ]\n",
      " [-2.4530582   0.9094646   2.0562832  -0.54099405]]\n",
      "\n",
      "Accuracy:  0.94269\n",
      "Precision:  [0.9525, 0.96216, 0.93612, 0.92021]\n",
      "Recall:  [0.96456, 0.89899, 0.95729, 0.95055]\n",
      "F1:  [0.95849, 0.9295, 0.94658, 0.93514]\n",
      "['comp.windows.x', 'sci.med', 'soc.religion.christian', 'talk.politics.guns']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./my_model\\pytorch_model.bin\n"
     ]
    }
   ],
   "source": [
    "## TRAINING\n",
    "\n",
    "from transformers import Trainer, TrainingArguments\n",
    "from sklearn.metrics import precision_recall_fscore_support, accuracy_score\n",
    "\n",
    "SOLUTION = 1\n",
    "\n",
    "# instantiate model\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "if SOLUTION == 1:\n",
    "    model = DistilBertForPostClassification(config=distilbert.config, num_labels=len(categories), freeze_encoder = True)\n",
    "else:\n",
    "    model = DistilBertForPostClassification(config=distilbert.config, num_labels=len(categories), freeze_encoder = True)\n",
    "\n",
    "# print info about model's parameters\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "model_parameters = filter(lambda p: p.requires_grad, model.parameters())\n",
    "trainable_params = sum([np.prod(p.size()) for p in model_parameters])\n",
    "    \n",
    "def compute_metrics(pred):\n",
    "    labels = pred.label_ids\n",
    "    preds = pred.predictions.argmax(-1)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds)\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    return {\n",
    "        'accuracy': acc,\n",
    "        'f1': f1,\n",
    "        'precision': precision,\n",
    "        'recall': recall\n",
    "    }\n",
    "\n",
    "\n",
    "lr = 0.00005\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',          \n",
    "    logging_dir='./logs',\n",
    "    logging_first_step=True,\n",
    "    logging_steps=50,\n",
    "    num_train_epochs=4,              \n",
    "    per_device_train_batch_size=8,  \n",
    "    learning_rate=lr,\n",
    "    weight_decay=0.01        \n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,                         \n",
    "    args=training_args,                  \n",
    "    train_dataset=train_dataset,         \n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "train_results = trainer.train()\n",
    "\n",
    "test_results = trainer.predict(test_dataset=test_dataset)\n",
    "\n",
    "def round_res(ml):\n",
    "    return [ round(elem,5) for elem in ml ]\n",
    "\n",
    "Accuracy= round(test_results.metrics['test_accuracy'], 5)\n",
    "Precision= round_res(test_results.metrics['test_precision'])\n",
    "Recall= round_res(test_results.metrics['test_recall'])\n",
    "F1 = round_res(test_results.metrics['test_f1'])\n",
    "\n",
    "print('Predictions: \\n', test_results.predictions)\n",
    "print('\\nAccuracy: ', Accuracy)\n",
    "print('Precision: ', Precision)\n",
    "print('Recall: ',Recall)\n",
    "print('F1: ', F1)\n",
    "print(categories)\n",
    "MODEL_PATH = './my_model'\n",
    "trainer.save_model(MODEL_PATH)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./my_model\n",
      "Configuration saved in ./my_model\\config.json\n",
      "Model weights saved in ./my_model\\pytorch_model.bin\n"
     ]
    }
   ],
   "source": [
    "MODEL_PATH = './my_model'\n",
    "trainer.save_model(MODEL_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading weights file ./my_model\\pytorch_model.bin\n",
      "loading configuration file https://huggingface.co/distilbert-base-uncased/resolve/main/config.json from cache at C:\\Users\\Laran/.cache\\huggingface\\transformers\\23454919702d26495337f3da04d1655c7ee010d5ec9d77bdb9e399e00302c0a1.91b885ab15d631bf9cee9dc9d25ece0afd932f2f5130eba28f2055b2220c0333\n",
      "Model config DistilBertConfig {\n",
      "  \"activation\": \"gelu\",\n",
      "  \"architectures\": [\n",
      "    \"DistilBertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"dim\": 768,\n",
      "  \"dropout\": 0.1,\n",
      "  \"hidden_dim\": 3072,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"distilbert\",\n",
      "  \"n_heads\": 12,\n",
      "  \"n_layers\": 6,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"qa_dropout\": 0.1,\n",
      "  \"seq_classif_dropout\": 0.2,\n",
      "  \"sinusoidal_pos_embds\": false,\n",
      "  \"tie_weights_\": true,\n",
      "  \"transformers_version\": \"4.14.1\",\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/distilbert-base-uncased/resolve/main/pytorch_model.bin from cache at C:\\Users\\Laran/.cache\\huggingface\\transformers\\9c169103d7e5a73936dd2b627e42851bec0831212b677c637033ee4bce9ab5ee.126183e36667471617ae2f0835fab707baa54b731f991507ebbb55ea85adb12a\n",
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_layer_norm.bias', 'vocab_transform.bias', 'vocab_transform.weight', 'vocab_layer_norm.weight', 'vocab_projector.bias', 'vocab_projector.weight']\n",
      "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of DistilBertModel were initialized from the model checkpoint at distilbert-base-uncased.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use DistilBertModel for predictions without further training.\n",
      "All model checkpoint weights were used when initializing DistilBertForPostClassification.\n",
      "\n",
      "All the weights of DistilBertForPostClassification were initialized from the model checkpoint at ./my_model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use DistilBertForPostClassification for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lung cancer is a deadly disease.\n",
      "\tProbabilities assigned by the model : \n",
      "\t\tcomp.windows.x : 0.0011246519861742854\n",
      "\t\tsci.med : 0.9846445322036743\n",
      "\t\tsoc.religion.christian : 0.006960222031921148\n",
      "\t\ttalk.politics.guns : 0.007270723581314087\n",
      "\n",
      "\t--> Prediction : sci.med\n",
      "------------------------------------------------\n",
      "\n",
      "God is love\n",
      "\tProbabilities assigned by the model : \n",
      "\t\tcomp.windows.x : 0.008614864200353622\n",
      "\t\tsci.med : 0.05478717386722565\n",
      "\t\tsoc.religion.christian : 0.9028907418251038\n",
      "\t\ttalk.politics.guns : 0.03370722755789757\n",
      "\n",
      "\t--> Prediction : soc.religion.christian\n",
      "------------------------------------------------\n",
      "\n",
      "How can you install Microsoft Office extensions?\n",
      "\tProbabilities assigned by the model : \n",
      "\t\tcomp.windows.x : 0.9807853698730469\n",
      "\t\tsci.med : 0.012270537205040455\n",
      "\t\tsoc.religion.christian : 0.0029387103859335184\n",
      "\t\ttalk.politics.guns : 0.004005391150712967\n",
      "\n",
      "\t--> Prediction : comp.windows.x\n",
      "------------------------------------------------\n",
      "\n",
      "Gun killings increase every year.\n",
      "\tProbabilities assigned by the model : \n",
      "\t\tcomp.windows.x : 0.0009757213410921395\n",
      "\t\tsci.med : 0.01681218482553959\n",
      "\t\tsoc.religion.christian : 0.017000965774059296\n",
      "\t\ttalk.politics.guns : 0.9652110934257507\n",
      "\n",
      "\t--> Prediction : talk.politics.guns\n",
      "------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "## PREDICTIONS\n",
    "\n",
    "model = DistilBertForPostClassification.from_pretrained(\n",
    "    './my_model', config=distilbert.config, num_labels=len(categories)).to(device)\n",
    "for sentence in ['Lung cancer is a deadly disease.', 'God is love', 'How can you install Microsoft Office extensions?', 'Gun killings increase every year.']:\n",
    "    encoding = tokenizer.encode_plus(sentence)\n",
    "    encoding['input_ids'] = torch.tensor([encoding.input_ids]).to(device)\n",
    "    encoding['attention_mask'] = torch.tensor(encoding.attention_mask).to(device)\n",
    "    out = model(**encoding)\n",
    "    categories_probability = torch.nn.functional.softmax(out[0], dim=1).flatten()\n",
    "    print(sentence)\n",
    "    print('\\tProbabilities assigned by the model : ')\n",
    "    for n,c in enumerate(categories):\n",
    "        print('\\t\\t{} : {}'.format(c, categories_probability[n]))\n",
    "    print('\\n\\t--> Prediction :', categories[categories_probability.argmax()])\n",
    "    print('------------------------------------------------\\n')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Intro)\n",
    "\n",
    "Here the results of all our experiment.\n",
    "The initial state is computing without fine tuning and 4 epoch,\n",
    "here the result of this initial state.\n",
    "\n",
    "\\begin{tabular}{| T | V |}\n",
    "Accuracy & 0.90663 \\\\\n",
    "Precision & [0.87244, 0.90556, 0.91525, 0.94135] \\\\\n",
    "Recall & [0.96962, 0.82323, 0.94975, 0.88187]  \\\\\n",
    "F1 & [0.91847, 0.86243, 0.93218, 0.91064] \\\\\n",
    "\\end{tabular}\n",
    "\n",
    "Now let's talk about the configuration of my computers.\n",
    "\n",
    "The tests were made on my GPU, who achieve a time of approximately 100 seconds for this basic example, instead of 7500 with my CPU.\n",
    "\n",
    "Let's now see what are the results of our experiments.\n",
    "\n",
    "1) Increase the numbers of epoch \n",
    "\n",
    "As said previously, the fact that i can computes with my GPU, help a lot for the computation times. \\\\\n",
    "Thanks to that, we can try multiples numbers of epoch.\\\\\n",
    "First we chose to increase just by one to see if there is a big change.\\\\\n",
    "Then we will follow the fibonacci principle and increase it by 2 and 3, to increase gradually the numbers of epoch. \\\\\n",
    "To resume, for the following example, we take as epoch 5, 7 and 10.\n",
    "\\begin{tabular}{ | T|C|S|D| }\n",
    "Epoch & 5 & 7 & 10 \\\\\n",
    "Accuracy & 0.9207 & 0.9311 & 0.9368 \\\\\n",
    "Precision & [0.9021, 0.9160, 0.9242, 0.9461] & [0.92289, 0.93048, 0.92944, 0.94334] & [0.9407, 0.9513, 0.9353, 0.9202] \\\\\n",
    "Recall & [0.9797, 0.8813, 0.9497, 0.8681] &[0.96962, 0.87879, 0.9598, 0.91484] & [0.9645, 0.8888, 0.9447, 0.9505] \\\\\n",
    "F1 & [0.9393, 0.8983, 0.9368, 0.9054] & [0.94568, 0.9039, 0.94438, 0.92887] &[0.9525, 0.9191, 0.94, 0.9351] \\\\\n",
    "\\end{tabular}\n",
    "\n",
    "We can see that when we increase the numbers of steps, the accuracy will increase too, our result are up to 1.7% more accurate.\n",
    "\n",
    "The biggest improvement were made between four and five epoch, where the accuracy is increase by 1.09%.\n",
    "But as we can see, between five epoch and ten epoch the values of the accuracy and the F1-mesure start to converge.\n",
    "\n",
    "\n",
    "2) Fine tune the model \n",
    "\n",
    "This a good starting point, now let's see what append when we use fine tuning, we can estimate that the model will be more efficient, because all the layers will be trained.\n",
    "\n",
    "\\begin{tabular}{ | T | C | }\n",
    "Accuracy & 0.9672 \\\\\n",
    "Precision & [0.9677, 0.9839, 0.9668, 0.9751]\\\\\n",
    "Recall & [0.9848, 0.9242, 0.98994, 0.9698]\\\\\n",
    "F1 & [0.9762, 0.9531, 0.9669, 0.9054] \\\\\n",
    "\\end{tabular}\n",
    "\n",
    "We can see that there is a big improvement, but that the running time is more longer, this can be explain by the fact that the back propagation will be applied in all the layer. But for the F1-mesure we can see that some values are lower than the initial case. The model seems overfit.\n",
    "\n",
    "3)\n",
    "We tried to make a lot of improvement.\n",
    "The first idea was to combine a large number of epoch and the fine tuning, but we saw that the accurate quickly converge to 0.97, and this is not improving the model.\n",
    "\n",
    "After this failure we chose to keep our initial state, so from now all the following experiment have been done with 4 epoch and no fine tuning. \n",
    "We will show you only the experiment who reach to an accuracy greater than 94%, we found the others non relevant since the initial case with 10 epoch almost reach it.  \n",
    "\n",
    "Our second idea was to adjusts the learning rate.\n",
    "We saw that when we are increasing the number of epoch the model tend to an higher value, the goal here is just to increase the learning rate to have good result even if the number of epoch is small.\n",
    "After doing a lot of experiment we chose to keep a learning rate of 0.005.\n",
    "Let's see our new result.\n",
    "\n",
    "\\begin{tabular}{ | T | C | }\n",
    "Accuracy &  0.950428 \\\\\n",
    "Precision &  [0.96222, 0.97035, 0.92399, 0.9478] \\\\\n",
    "Recall & [0.96709, 0.90909, 0.97739, 0.9478] \\\\\n",
    "F1 &  [0.96465, 0.93872, 0.94994, 0.9478] \\\\\n",
    "\\end{tabular}\n",
    "\n",
    "Our final idea was to add a layer in the model before the classifier, we saw that there is a lot of variance in the observation. The goal here is to apply a mask to normalize the data. To do it we made a pre classifier with only one layer of size input x input.  \n",
    "Let's see the result of this experiment\n",
    "\n",
    "\n",
    "\\begin{tabular}{ | T | C | }\n",
    "Accuracy &  0.94527 \\\\\n",
    "Precision &  [0.96438, 0.96467, 0.91335, 0.94247] \\\\\n",
    "Recall &  [0.95949, 0.89646, 0.9799, 0.94505] \\\\\n",
    "F1 & [0.96193, 0.92932, 0.94545, 0.94376] \\\\\n",
    "\\end{tabular}\n",
    "\n",
    "As we can see that precision values have less variance, that was what we want, but the accuracy is lower than expected .\n",
    "\n",
    "4) Conclusion\n",
    "\n",
    "Now we want to make a point on what we learn during this pratical.\n",
    "\n",
    "First of all we learn the pratical part of building a machine learning architecture, and all the steps we have to do, how to change the numbers of steps, the learning rate ... \n",
    "\n",
    "Secondly we saw how to Fine tune a model and how really is the improvement. \n",
    "\n",
    "To finish we also learned a lot of things when we try made the classification layers. \n",
    "\n",
    "Our first idea was to make a lot of hidden layers and we saw that the results was worst the basic result.\n",
    "\n",
    "So we think about mixing fine tuning and a lot of layers classification, to improve this result, it was clearly not significant.\n",
    "\n",
    "And to finish when we saw that a simple layer is enought to grant great performance, we conclude that so the performance will not depend of numbers of layers.\n",
    "\n",
    "This is the most important point we learned in this pratical, even with a high number of steaps, increasing the numbers of layer is not always a good solution, and can be just negative for the architecture.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "Untitled1.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
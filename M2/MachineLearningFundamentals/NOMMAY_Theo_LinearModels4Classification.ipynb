{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Perceptron, Adaline and Logistique Regression\n",
    "NOMMAY Theo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are interested in the implementation of the perceptron algorithm (Rosenblatt, 68), Adaline (Widrow et Hoff, 60) and Logisitc Regression (Cox, 66) whose pseudo-code are the following:\n",
    "\n",
    "Perceptron:\n",
    "`Input: Train, eta, MaxEp\n",
    "init: w\n",
    "epoch = 0\n",
    "err = 1\n",
    "m = len(Train)\n",
    "while epoque <= MaxEp and err! = 0\n",
    "    err = 0\n",
    "    for i in 1: m\n",
    "        choose randomly an example (x,y)\n",
    "        h <- w * x\n",
    "        if (y * h <= 0)\n",
    "            w <- w + eta * y * x\n",
    "            err <- err + 1\n",
    "     epoch <- epoch + 1\n",
    "output: w`\n",
    "\n",
    "Adaline:\n",
    "`input: Train, eta, MaxEp\n",
    "init : w\n",
    "epoque=0\n",
    "err=1\n",
    "m = len(Train)\n",
    "while epoque<=MaxEp and err!=0\n",
    "    err=0\n",
    "    for i in 1:m\n",
    "        choose randomly an example (x,y)\n",
    "        h <- w*x\n",
    "        if(y*h<=0)\n",
    "           err <- err+1\n",
    "        w <- w + eta*(y-dp)*x\n",
    "     epoque <- epoque+1\n",
    "output: w`\n",
    "\n",
    "Logistic Regression:\n",
    "`input: Train, eta, MaxEp\n",
    "init : w\n",
    "epoque=0\n",
    "err=1\n",
    "m = len(Train)\n",
    "while epoque<=MaxEp and err!=0\n",
    "    err=0\n",
    "    for i in 1:m\n",
    "        choose randomly an example (x,y)\n",
    "        h <- w*x\n",
    "        if(y*h<=0)\n",
    "           err <- err+1\n",
    "        w <- w + eta*y*(1-sigm(y*dp))*x\n",
    "     epoque <- epoque+1\n",
    "output: w`\n",
    "\n",
    "1. Create a list of 4 elements corresponding to the logical AND example called `Train`:\n",
    "$Train=\\{((1,+1,+1),+1),((1,-1,+1),-1),((1,-1,-1),-1),((1,+1,-1),-1)\\}$\n",
    "\n",
    "Each element of the list is a list which last characteristic is the class of the example and the first characteristics their coordinates with the biais '1' included at the beginning of each vector.\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "Train=[[1,1,1,1],\n",
    "       [1,-1,1,-1],\n",
    "       [1,-1,-1,-1],\n",
    "       [1,1,-1,-1]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "We made a class \" Tools \", who contains all external functions, with his documentation below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class Tools :\n",
    "\n",
    "    def dot_product(X, Y):\n",
    "        '''\n",
    "        input : X and Y two vectors to be multiply\n",
    "        output : a Float, result of the dot product between X and Y\n",
    "        raise an exception if X and Y have not the same dimension\n",
    "        '''\n",
    "        d = len(Y)\n",
    "        if d != len(X):\n",
    "            raise Exception(\"The vectors don't have the same dimension\")\n",
    "        res = sum([xj * yj for xj, yj in zip(X, Y)])\n",
    "        return res\n",
    "\n",
    "\n",
    "    def get_shuffled_index(size):\n",
    "        '''\n",
    "        input :size, the size of the output\n",
    "        output : a shuffle list with all the elements between 0 and size\n",
    "        '''\n",
    "        l = list(range(size))\n",
    "        shuffle(l)\n",
    "        return l\n",
    "\n",
    "\n",
    "\n",
    "    def sigmoid(z):\n",
    "        '''\n",
    "        input : z the exposant\n",
    "        output : a float who represent the result of the sigmoid fonction with z\n",
    "        '''\n",
    "\n",
    "        return 1.0 / (1.0 + exp(-z))\n",
    "\n",
    "    def mean_empirical_risk(test, train, *args, eta=0.01, maxep=500, nb_iter = 10000):\n",
    "        '''\n",
    "        input : the testing set,\n",
    "                the training set,\n",
    "                a list of function,\n",
    "                the value of eta (by default 0.01),\n",
    "                the value of max epoch (by default 500),\n",
    "                the number of iteration (by default 10000),\n",
    "\n",
    "\n",
    "        output : a dictionary who match a function and the mean of all his errors\n",
    "        '''\n",
    "\n",
    "        result = {}\n",
    "        for function in args:\n",
    "            s = 0\n",
    "            for _ in range(nb_iter) :\n",
    "                s += EmpiricalRisk(test, function(train, eta= eta, MaxEp=maxep))\n",
    "            result[function.__name__] = s/nb_iter\n",
    "        return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class Tools in module __main__:\n",
      "\n",
      "class Tools(builtins.object)\n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  dot_product(X, Y)\n",
      " |      input : X and Y two vectors to be multiply\n",
      " |      output : a Float, result of the dot product between X and Y\n",
      " |      raise an exception if X and Y have not the same dimension\n",
      " |  \n",
      " |  get_shuffled_index(size)\n",
      " |      input :size, the size of the output\n",
      " |      output : a shuffle list with all the elements between 0 and size\n",
      " |  \n",
      " |  mean_empirical_risk(test, train, *args, eta=0.01, maxep=500, nb_iter=10000)\n",
      " |      input : the testing set,\n",
      " |              the training set,\n",
      " |              a list of function,\n",
      " |              the value of eta (by default 0.01),\n",
      " |              the value of max epoch (by default 500),\n",
      " |              the number of iteration (by default 10000),\n",
      " |      \n",
      " |      \n",
      " |      output : a dictionary who match a function and the mean of all his errors\n",
      " |  \n",
      " |  sigmoid(z)\n",
      " |      input : z the exposant\n",
      " |      output : a float who represent the result of the sigmoid fonction with z\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors defined here:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(Tools)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Code the Perceptron, Adaline and LR (Logistic regression) programs\n",
    "\n",
    "Hint: You can write a function that calculates the dot product between an example $\\mathbf{x} = (x_1, \\ldots, x_d)$ and the weight vector $\\mathbf{w} = (w_0, w_1, \\ldots, w_d)$:\n",
    "$ h(\\mathbf{x},\\mathbf{w}) = w_0 + \\sum_ {j = 1} ^ d w_j x_j $.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import exp\n",
    "from random import shuffle\n",
    "\n",
    "def h(x,w):\n",
    "    w0, *w = w\n",
    "    return Tools.dot_product(w, x) + w0\n",
    "\n",
    "def Perceptron(Train,eta=0.01,MaxEp=500):\n",
    "    # Perceptron Algorithm \n",
    "    dimension=len(Train[0])-1\n",
    "    m=len(Train)\n",
    "    W=[0.0 for _ in range(dimension)]\n",
    "    epoch = 0\n",
    "    err = 1\n",
    "    while epoch < MaxEp and not err == 0:\n",
    "        err = 0\n",
    "        for i in Tools.get_shuffled_index(m):\n",
    "            *antecedant, image = Train[i] #split train to get the x and y values\n",
    "            h = Tools.dot_product(W, antecedant)\n",
    "            if image * h <= 0:\n",
    "                for j in range(0, dimension) :\n",
    "                    W[j] += eta * image * antecedant[j]\n",
    "                err += 1\n",
    "            \n",
    "        epoch += 1\n",
    "    return W\n",
    "\n",
    "\n",
    "def Adaline(Train,eta=0.01,MaxEp=500):\n",
    "    # Adaline Algorithm\n",
    "    dimension=len(Train[0])-1\n",
    "    m=len(Train)\n",
    "    W=[0.0 for _ in range(dimension)]\n",
    "    epoch = 0\n",
    "    err = 1\n",
    "    while epoch < MaxEp and not err == 0:\n",
    "        err = 0\n",
    "        for i in Tools.get_shuffled_index(m):\n",
    "            *antecedant, image = Train[i] #split train to get the x and y values\n",
    "            dp = Tools.dot_product(W, antecedant)\n",
    "            if image * dp <= 0:\n",
    "                    err += 1\n",
    "            bias, *x = antecedant\n",
    "            sd = h(x,W)\n",
    "            for j in range(0, dimension) :\n",
    "                W[j] += eta * (image - sd) * antecedant[j]\n",
    "            \n",
    "        epoch += 1\n",
    "    return W\n",
    "\n",
    "def LR(Train,eta=0.01,MaxEp=500):\n",
    "    # Logisitc Regression Algorithm \n",
    "    dimension=len(Train[0])-1\n",
    "    m=len(Train)\n",
    "    W=[0.0 for _ in range(dimension)]\n",
    "    epoch = 0\n",
    "    err = 1\n",
    "    while epoch < MaxEp and not err == 0:\n",
    "        err = 0\n",
    "        for i in Tools.get_shuffled_index(m):\n",
    "            *antecedant, image = Train[i] #split train to get the x and y values\n",
    "            dp = Tools.dot_product(W, antecedant)\n",
    "            if image * dp <= 0:\n",
    "                err += 1\n",
    "            bias, *x = antecedant #split X to get the biais and x values\n",
    "            sd = h(x,W)\n",
    "            for j in range(0, dimension) :\n",
    "                W[j] += eta * image * (1 -Tools.sigmoid(image * sd)) * antecedant[j]\n",
    "\n",
    "        epoch += 1\n",
    "\n",
    "    return W\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Apply the three learning models on the logical AND, and calculate the model error rate on this basis.\n",
    "\n",
    "Hint: You can write a function that takes a weight vector $\\mathbf{w}$ and an example $(\\mathbf{x},y)$ and calculates the error rate of the model with weight $\\mathbf{w}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Perceptron': 0.0, 'Adaline': 0.0, 'LR': 0.0}\n"
     ]
    }
   ],
   "source": [
    "def EmpiricalRisk(Test,W):\n",
    "    E=0.0\n",
    "    m=len(Test)\n",
    "    # The empirical error of a model with weight W on a test set of size m\n",
    "    for Obs in Test:\n",
    "        y = Obs[-1]\n",
    "        h_w = h(Obs[:-1], W)\n",
    "        if y*h_w <= 0:\n",
    "            E+=1.0\n",
    "    return E/float(m)\n",
    "\n",
    "AND = [[1,1,1],\n",
    "       [-1,-1,-1],\n",
    "       [-1,1,-1],\n",
    "       [-1,-1,-1]]\n",
    "\n",
    "print(Tools.mean_empirical_risk(AND, Train, Perceptron, Adaline, LR))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. We are now going to focus on the behavior of the three models on http://archive.ics.uci.edu/ml/datasets/connectionist+bench+(sonar,+mines+vs.+rocks), https://archive.ics.uci.edu/ml/datasets/spambase, https://archive.ics.uci.edu/ml/datasets/Breast+Cancer+Wisconsin+%28Original%29, https://archive.ics.uci.edu/ml/datasets/Ionosphere. These files are in the current respository with the names `sonar.txt`; `spam.txt`; `wdbc.txt` and `ionoshpere.txt`. We can use the following `ReadCollection` function in order to read the files in the form of the training set that is requested. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import sqrt\n",
    "import pandas as pd\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def Normalize(x):\n",
    "    norm=0.0\n",
    "    for e in x:\n",
    "        norm+=e**2\n",
    "    for i in range(len(x)):\n",
    "        x[i]/=sqrt(norm)\n",
    "    return x\n",
    "\n",
    "def ReadCollection(filename):\n",
    "    tag_df=pd.read_table(filename,sep=',',header=None)\n",
    "    if \"wdbc\" in filename:\n",
    "        Dic={'M': -1, 'B': +1}\n",
    "    elif \"sonar\" in filename:\n",
    "        Dic={'R': -1, 'M': +1}\n",
    "    elif(\"iono\" in filename):\n",
    "        Dic={'g': -1, 'b': +1}\n",
    "    elif(\"spam\" in filename):\n",
    "        Dic={0:-1, 1:+1}\n",
    "        \n",
    "    X=[]\n",
    "    for e in range(len(tag_df)):\n",
    "        x=list(tag_df.loc[e,:])\n",
    "        if \"wdbc\" in filename :\n",
    "            x.pop(0)\n",
    "            cls=x.pop(0)\n",
    "        else:\n",
    "            cls=x.pop()\n",
    "        x= Normalize(x)\n",
    "        x.insert(len(x),Dic[cls])\n",
    "        X.append(x)\n",
    "    random.shuffle(X)\n",
    "\n",
    "    return X\n",
    "\n",
    "wdbc_col = ReadCollection(\"wdbc.txt\")\n",
    "sonar_col = ReadCollection(\"sonar.txt\")\n",
    "iono_col = ReadCollection(\"ionosphere.txt\")\n",
    "spam_col = ReadCollection(\"spam.txt\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " 2. Run the three models on these files with $\\eta=0.01$ et $\\eta=0.1$ and `MaxEp=500`.\n",
    " \n",
    " 3. Report in the table below the average of the error rates on the test by repeating each experiment 10 times. \n",
    " \n",
    " <br>\n",
    " <br>\n",
    " \n",
    " \n",
    " <center> $\\eta=0.01$, MaxE$=500$ </center>\n",
    "    \n",
    "    \n",
    "  | Collection | Perceptron | Adaline |    RL    |\n",
    "  |------------|------------|---------|----------|\n",
    "| WDBC | 0.0997 | 0.0874 | 0.09336 |\n",
    "| Ionosphere | 0.1307 | 0.0989 | 0.07216 |\n",
    "| Sonar | 0.2865 | 0.2615 | 0.2692 |\n",
    "| Spam | 0.1815 | 0.2557 | 0.2275 |\n",
    " \n",
    " <br><br>\n",
    "  \n",
    "  <center> $\\eta=0.1$, MaxEp$=500$ </center>\n",
    "    \n",
    "    \n",
    "  | Collection | Perceptron | Adaline |    RL    |\n",
    "  |------------|------------|---------|----------|\n",
    "| WDBC | 0.1133 | 0.0979 | 0.09126|\n",
    "| Ionosphere | 0.1068 | 0.1023 | 0.07557|\n",
    "| Sonar | 0.2798 | 0.2760 | 0.226|\n",
    "| Spam | 0.2182 | 0.2785 | 0.1682|\n",
    "\n",
    " <br><br>\n",
    "  Hint: you can use the following function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eta=0.01\n",
      "-------\n",
      "| WDBC | 0.0997 | 0.0874 | 0.09336\n",
      "| Ionosphere | 0.1307 | 0.0989 | 0.07216\n",
      "| Sonar | 0.2865 | 0.2615 | 0.2692\n",
      "| Spam | 0.1815 | 0.2557 | 0.2275\n",
      "eta=0.1\n",
      "-------\n",
      "| WDBC | 0.1133 | 0.0979 | 0.09126\n",
      "| Ionosphere | 0.1068 | 0.1023 | 0.07557\n",
      "| Sonar | 0.2798 | 0.2760 | 0.226\n",
      "| Spam | 0.2182 | 0.2785 | 0.1682\n"
     ]
    }
   ],
   "source": [
    "N = 20\n",
    "maxep = 500\n",
    "for eta in [0.01,0.1]:\n",
    "    print(f'eta={eta}\\n-------')\n",
    "    for name, X in zip([\"WDBC\", \"Ionosphere\", \"Sonar\", \"Spam\"],[wdbc_col,iono_col,sonar_col,spam_col]):\n",
    "        errP=errA=errL=0.0\n",
    "        for i in range(N):\n",
    "            x_train ,x_test = train_test_split(X,test_size=0.25)\n",
    "            x_train = [[1] + x for x in x_train] #Adding the biais in our training set\n",
    "            WLP=Perceptron(x_train,eta,maxep)\n",
    "            errP+=EmpiricalRisk(x_test,WLP)\n",
    "            WLA=Adaline(x_train,eta,maxep)\n",
    "            errA+=EmpiricalRisk(x_test,WLA)\n",
    "            WLR=LR(x_train,eta,maxep)\n",
    "            errL+=EmpiricalRisk(x_test,WLR)\n",
    "        print(f\"| {name} | {errP/float(N):.4f} | {errA/float(N):.4f} | {errL/float(N):0.4}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " 4. Normalize the vector representations of observations by dividing them with their norm and repeat questions 2 and 3. Are there any significant change by normalizing? Please explain.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Resp:</b> <br>\n",
    "When we remove the normalisation of x, python raise an overflow error, to analyse it, we will see in a first time what happend in our functions, then we will see how the normalize function is useful here\n",
    "\n",
    "<h2> I - What happend </h2>\n",
    "\n",
    "We know that the update function is :\n",
    "w = w + k * x * eta, with : <br> \n",
    "with $k \\in [y, (y -hw) , (1 - sigmoid(y * hw)) * y] $ <br>\n",
    "in this exemple eta is not very important, let's set it to 1 <br>\n",
    "so we have : <br> w = w + k * x * eta $\\Leftrightarrow$ w = w + k * x <br>\n",
    "Since x is not normalize, we will suppose that $\\forall_{i \\in{0,d}} |x[i]| > 1$  <br>\n",
    "We now have to analyse the value of k : \n",
    "<br> \n",
    "<br>\n",
    "<li> For perceptron, |k| = |y|, $\\Leftrightarrow$ |k| = 1, so while w and k * x have the same sign, the value of w will approach +/- infinity.\n",
    "<li> For Adaline and logistical regression, k depends on hw(x) who depend on the values of x. If x has big values, then the result of hw(x) can be very large  <br>\n",
    "\n",
    "    \n",
    "   So we can conclude that, with a lot of iteration, the value of |w| and/or hw(x) can be large, that's why python raise a warning. In the logistic regression, we need to compute the exponential of +/- hw(x), this value is too big to be compute, that's why python raise an error and stop the program <br> <br>\n",
    "A naive solution can be to decrease the maximum number of iteration, but it's clearly not the best solution<br>\n",
    "    <li> The number of iteration can be insufficient\n",
    "    <li> Even if the number of iteration is sufficient, the values in our training set can have a high standard deviation, if one value is much bigger than others, it will be hard to compute w correctly. \n",
    "    \n",
    "    \n",
    "The best solution is to find a function for the vector x, such that all the values are between 0 and 1. <br> But there is another problem. Let's define a function f(x), who replaces all the values of x with random values between 0 and 1. Now x is completely random and y depends on the old value of x, it's impossible to train w correctly. So the function must keep the same ratio between all the values of x.\n",
    "\n",
    "            \n",
    "            \n",
    "<h2> II - The normalize function</h2>\n",
    "<br>\n",
    "The normalize function divide the value of x[i] by the norm of x, so we have : <br>\n",
    "$x[i] = x[i]$ / $||x||$ $\\forall_{ i \\in [0, d]}$ <br>\n",
    "            \n",
    "\n",
    "with : $||x|| = \\sqrt{\\sum_{n = 0} ^ {d} x[n]²} $ \n",
    "<br>\n",
    "\n",
    "We know that $\\forall{x_1, x_2}$ $\\exists k$ such as : $x_1 / x_2 = (x_1 / K) / (x_2 / K) $ <br>\n",
    "But with the normalize function $x[i] = x[i]$ / $||x||$, here $K = ||x||$ we can conclude that the ratio between all elements in x are keeped after normalisation\n",
    "            \n",
    "<br>\n",
    "            \n",
    "\n",
    "To prove that all elements of x are between 0 and 1 we have to rewrite ||x|| to: <br>\n",
    "$\\forall i _{\\in0, d} : ||x|| = \\sqrt{x[i]² + \\sum_{n = 0} ^ {i - 1} x[n]² + \\sum_{n = i + 1} ^ d x[n]²} $\n",
    "<br> \n",
    "\n",
    "\n",
    "we can rewrite the normalize fonction : <br>\n",
    "$x[i] = x[i] / \\sqrt{(x[i]² + \\sum_{n = 0} ^ {i - 1} x[n]² + \\sum_{n = i + 1} ^ d x[n]²)}$  $\\forall i _{\\in0, d}$ <br>\n",
    "$\\Leftrightarrow$ <br>\n",
    "$x[i] = \\sqrt{ x[i]² /(x[i]² + \\sum_{n = 0} ^ {i - 1} x[n]² + \\sum_{n = i + 1} ^ d x[n]² )}$  $\\forall i _{\\in0, d}$ <br>\n",
    "\n",
    "since $x[i]² < x[i]² + \\sum_{n = 0} ^ {i - 1} x[n]² + \\sum_{n = i + 1} ^ d x[n]²$, and since the result of the square root is always positive, we can conclude that after normalisation: <br>\n",
    "        $\\forall_{ i \\in [0, d]}$ : $0 < x[i] < 1$  <br>\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
